{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 156 HW 5\n",
    "### Ankush Hommerich-Dutt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **[c]**\n",
    "\n",
    "E_in = $\\sigma^2(1 - \\frac{d+1}{N})$  \n",
    "\n",
    "If we plug in $\\sigma = 0.1, d = 8$ and E_in > 0.008, we get:\n",
    "\n",
    "$(0.1)^2(1 - \\frac{9}{N}) > 0.008$. Solving this for $N$:\n",
    "\n",
    "$1 - \\frac{9}{N} > 0.8 \\longrightarrow N > \\frac{9}{0.8} = 45$\n",
    "\n",
    "So $N$ has to be bigger than 45.\n",
    "\n",
    "Among the answer choices, 100 is the smallest number that is bigger than 45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **[d]**\n",
    "\n",
    "We see that for large $x_1$, we see that the classification is -1, and for large $x_2$, the classification is +1. Therefore, if $w_1$ is negative, then for large $x_1$, we will get that $w^Tx$ is negative, and if $w_2$ is positive, then for large $x_2$, we will get that $w^Tx$ is positive.\n",
    "\n",
    "This corresponds to answer choice d, where $w_1 < 0; w_2 > 0$\n",
    "\n",
    "(Of course all of the w should have a tilde above them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **[d]**\n",
    "\n",
    "We know that a linear model in a $d$ dimensional space has a VC dimension of $d + 1$ (this was proven in class for the perceptron). Therefore, in the transformed space, the dimension is 15, and so the linear model would have a VC dimension of 16. 20 is the smallest number amongst the answer choices that is larger than 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **[e]**\n",
    "\n",
    "We can use the chain rule to take the partial derivative. We see that it is answer choice e. I am not sure what work to show for this, as it is a one step process. The 2 goes to the front, we keep our original function that was in parenthesis, and then we multiply it by the derivative of that function with respect to (wrt) u. The derivative of $ue^v$ wrt $u$ is $e^v$, and the derivative of $-2ve^{-u}$ wrt $u$ is $2ve^{-u}$. This all corresponds to answer choice e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations:  10\n",
      "final u and v:  [0.04473629 0.02395871]\n",
      "the error for coordinate descent:  0.13981379199615315\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def error_func(u, v):\n",
    "    return (u*np.exp(v) - 2*v*np.exp(-u))**2\n",
    "\n",
    "def error_gradient(u, v):\n",
    "    return np.array([(u*np.exp(v) - 2*v*np.exp(-u))*(np.exp(v) + 2*v*np.exp(-u))*2, \\\n",
    "                     (u*np.exp(v)  - 2*v*np.exp(-u))*(u * np.exp(v) - 2*np.exp(-u))*2])\n",
    "\n",
    "\n",
    "def converge(weights, error, eta):\n",
    "    num_iter = 0\n",
    "    while error_func(weights[0], weights[1]) > error:\n",
    "        weights = weights - (eta * error_gradient(weights[0], weights[1]))\n",
    "        num_iter += 1 \n",
    "    return weights, num_iter      \n",
    "\n",
    "def coordinate_descent(weights, iterations, eta):\n",
    "    for i in range(iterations):\n",
    "        error = error_gradient(weights[0], weights[1]) \n",
    "        weights = weights - (eta * np.array([error[0], 0]))\n",
    "        \n",
    "        error = error_gradient(weights[0], weights[1]) \n",
    "        weights = weights - (eta * np.array([0, error[1]]))\n",
    "    \n",
    "    return error_func(weights[0], weights[1])\n",
    "\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    weights = np.array([1,1])\n",
    "    new_weights, iters = converge(weights, 10**(-14), 0.1)\n",
    "    print(\"number of iterations: \", iters)\n",
    "    print(\"final u and v: \", new_weights)\n",
    "    error_descent = coordinate_descent(weights, 15, 0.1)\n",
    "    print(\"the error for coordinate descent: \", error_descent)\n",
    "                          \n",
    "main() \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **[d]**\n",
    "\n",
    "As we can see, it took 10 iterations to converge to an error below $10^{-14}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **[e]**\n",
    "\n",
    "After the convergence based on the criteria above, we can see our final $u$ is 0.045 and our final $v$ is 0.024. This corresponds to answer choice e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **[a]**\n",
    "\n",
    "After 15 full iterations of coordinate descent, we see that the error is 0.14, which corresponds closest to answer choice a of $10^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 100: \n",
      "E_out:  0.10132224456326522\n",
      "num_epochs:  337.59\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.val = np.array([1, x, y])\n",
    "        \n",
    "    def outcome(self, line):\n",
    "        target = line.evaluate(self.val[1])\n",
    "        if self.val[2] > target:\n",
    "            return 1\n",
    "        return -1\n",
    "        \n",
    "class Line:\n",
    "    def __init__(self, point1, point2):\n",
    "        self.slope = (point2.val[2] - point1.val[2]) / (point2.val[1] - point1.val[1])\n",
    "        self.intercept = (self.slope * (-point1.val[1])) + point1.val[2]\n",
    "        \n",
    "    def evaluate(self, x):\n",
    "        return (self.slope * x) + self.intercept\n",
    "    \n",
    "        \n",
    "def random_point():\n",
    "    return Point(random.uniform(-1,1), random.uniform(-1,1))\n",
    "\n",
    "def target_function():\n",
    "    point1 = random_point()\n",
    "    point2 = random_point()\n",
    "    return Line(point1, point2)\n",
    "\n",
    "\n",
    "def iteration(outcomes, weight_vec, eta, n):\n",
    "    old_weight = weight_vec\n",
    "    perms = np.random.permutation(n)\n",
    "    for i in perms:\n",
    "        grad = -(outcomes[i][0].val * outcomes[i][1]) / \\\n",
    "        (1 + np.exp(weight_vec.dot(outcomes[i][0].val) * outcomes[i][1]))\n",
    "        weight_vec = weight_vec - (eta * grad)\n",
    "    \n",
    "    if LA.norm(weight_vec - old_weight) < 0.01:\n",
    "        return weight_vec, True\n",
    "    return weight_vec, False\n",
    "\n",
    "    \n",
    "def main(n):\n",
    "    target = target_function()\n",
    "    data = [random_point() for x in range(n)]\n",
    "    outcomes =  list(map(lambda x: (x,x.outcome(target)), data))\n",
    "    \n",
    "    eta = 0.01\n",
    "    converge = False \n",
    "    num_epochs = 0\n",
    "    w = np.array([0,0,0])\n",
    "    while converge != True:\n",
    "        w, converge = iteration(outcomes, w, eta, n)\n",
    "        num_epochs += 1  \n",
    "        \n",
    "    new_data = [random_point() for x in range(1000)]\n",
    "    new_outcomes =  list(map(lambda x: (x, x.outcome(target)), new_data))\n",
    "    E_out_sum = 0\n",
    "    for i in range(1000):\n",
    "        E_out_sum += np.log(1 + np.exp(w.dot(new_outcomes[i][0].val) * new_outcomes[i][1] * -1))\n",
    "    E_out = E_out_sum / 1000\n",
    "    return E_out, num_epochs\n",
    "\n",
    "test = [main(100) for i in range(100)]\n",
    "print(\"N = 100: \")\n",
    "print(\"E_out: \", sum(i for i, _ in test) / 10w0)\n",
    "print(\"num_epochs: \", sum(i for _, i in test) / 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **[d]**\n",
    "\n",
    "We get an E_out of 0.101 with N = 100, which corresponds closest to answer choice d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **[a]**\n",
    "\n",
    "We converged after 337 epochs on average, which corresponds to answer choice a of 350."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **[b]**\n",
    "\n",
    "We see that in the PLA, the weight vector after picking one point is updated by $w = w + x_ny_n$, and so $x_ny_n$ needs to equal -$\\nabla e_n$ to draw an analogy to stochastic gradient descent (we can ignore eta since it is a constant). If we take the gradient of answer choice b, we get $\\nabla e_n = -x_ny_n$, which is exactly what we needed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
